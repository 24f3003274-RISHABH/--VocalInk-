{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwGAE6Qh0Mi9g2Ht9U2tgm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishab8218/--VocalInk-/blob/main/Mavericks_Whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n",
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suc9HW9KKhYx",
        "outputId": "60c28079-4706-4249-a62a-dc778c268ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m727.0/803.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=35bafa2b9b307d8cbf081d6803d2944eaaa1781072fd8d588b2b600371a9cd33\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPA0H3xN6z9H",
        "outputId": "8e857cfd-6ab6-4249-cd7c-6028e834c5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,233 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,586 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,600 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,969 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,864 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Fetched 38.4 MB in 4s (9,472 kB/s)\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 80 not upgraded.\n",
            "Required libraries and ffmpeg installed.\n"
          ]
        }
      ],
      "source": [
        "# Install whisper and pydub (if not already installed)\n",
        "!pip install -q whisper pydub\n",
        "\n",
        "# Ensure ffmpeg is installed, which is essential for pydub and whisper\n",
        "!sudo apt-get update -q && sudo apt-get install -y ffmpeg -q\n",
        "\n",
        "print(\"Required libraries and ffmpeg installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "import io\n",
        "import datetime\n",
        "\n",
        "# --- JavaScript for Audio Recording ---\n",
        "# This JavaScript code creates a recording interface and sends the audio data back to Python.\n",
        "# It records in WAV format.\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = () => resolve(reader.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = async (time) => {\n",
        "  let stream       = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  let recorder     = new MediaRecorder(stream)\n",
        "  let div          = document.createElement('div')\n",
        "  let start        = document.createElement('button')\n",
        "  let stop         = document.createElement('button')\n",
        "  let recordedChunks = []\n",
        "  start.textContent = 'Start Recording'\n",
        "  stop.textContent  = 'Stop Recording'\n",
        "  div.appendChild(start)\n",
        "  div.appendChild(stop)\n",
        "  document.body.appendChild(div)\n",
        "\n",
        "  return new Promise(async resolve => {\n",
        "    start.onclick = () => {\n",
        "      recorder.start()\n",
        "      start.textContent = 'Recording...'\n",
        "      start.disabled = true;\n",
        "      stop.disabled = false;\n",
        "    }\n",
        "    stop.onclick =  () => {\n",
        "      recorder.stop()\n",
        "      start.textContent = 'Start Recording'\n",
        "      start.disabled = false;\n",
        "      stop.disabled = true;\n",
        "    }\n",
        "    recorder.ondataavailable = e => recordedChunks.push(e.data)\n",
        "    recorder.onstop = async () => {\n",
        "      let blob = new Blob(recordedChunks, { type: 'audio/wav' })\n",
        "      let url  = await b2text(blob)\n",
        "      resolve(url)\n",
        "    }\n",
        "  });\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def record_audio_colab(filename=\"recorded_audio.wav\"):\n",
        "    \"\"\"\n",
        "    Displays a record/stop button, records audio from browser, and saves it as WAV.\n",
        "    Returns the filename of the recorded audio.\n",
        "    \"\"\"\n",
        "    print(\"Press 'Start Recording' to begin, 'Stop Recording' to finish.\")\n",
        "    display(Javascript(RECORD))\n",
        "    audio_url = eval_js('record(1)') # The '1' is a dummy parameter for JS, actual recording duration is user controlled\n",
        "    data = b64decode(audio_url.split(',')[1])\n",
        "\n",
        "    # Generate a unique filename using timestamp\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_filename = f\"recorded_audio_{timestamp}.wav\"\n",
        "\n",
        "    with open(output_filename, 'wb') as f:\n",
        "        f.write(data)\n",
        "    print(f\"Audio saved as '{output_filename}'\")\n",
        "    return output_filename\n",
        "\n",
        "# Optional: Add a CSS style for better button appearance\n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        "button {\n",
        "  background-color: #4CAF50; /* Green */\n",
        "  border: none;\n",
        "  color: white;\n",
        "  padding: 15px 32px;\n",
        "  text-align: center;\n",
        "  text-decoration: none;\n",
        "  display: inline-block;\n",
        "  font-size: 16px;\n",
        "  margin: 4px 2px;\n",
        "  cursor: pointer;\n",
        "  border-radius: 8px;\n",
        "}\n",
        "button:hover {\n",
        "  background-color: #45a049;\n",
        "}\n",
        "button:disabled {\n",
        "  background-color: #cccccc;\n",
        "  cursor: not-allowed;\n",
        "}\n",
        "</style>\n",
        "\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0XMhgydi62fy",
        "outputId": "0a162ae1-346e-419d-ef7f-096b7ee9d497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "button {\n",
              "  background-color: #4CAF50; /* Green */\n",
              "  border: none;\n",
              "  color: white;\n",
              "  padding: 15px 32px;\n",
              "  text-align: center;\n",
              "  text-decoration: none;\n",
              "  display: inline-block;\n",
              "  font-size: 16px;\n",
              "  margin: 4px 2px;\n",
              "  cursor: pointer;\n",
              "  border-radius: 8px;\n",
              "}\n",
              "button:hover {\n",
              "  background-color: #45a049;\n",
              "}\n",
              "button:disabled {\n",
              "  background-color: #cccccc;\n",
              "  cursor: not-allowed;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "print(os.getcwd())\n",
        "!ls -l\n",
        "def transcribe_recorded_audio():\n",
        "    audio_file = record_audio_colab()\n",
        "    if not os.path.exists(audio_file):\n",
        "        print(\"No audio file was recorded or found.\")\n",
        "        return\n",
        "\n",
        "    # Load the Whisper model\n",
        "    print(\"\\nLoading Whisper model...\")\n",
        "    # Consider using a smaller model like \"tiny\" or \"base\" for faster processing\n",
        "    # \"medium\" or \"large\" offer better accuracy but take longer to load and run\n",
        "    model = whisper.load_model(\"base\")\n",
        "    print(\"Whisper model loaded.\")\n",
        "\n",
        "    # Transcribe the recorded audio\n",
        "    print(f\"\\nTranscribing '{audio_file}' with Whisper...\")\n",
        "    try:\n",
        "        result = model.transcribe(audio_file)\n",
        "        print(\"\\n--- Transcription Result ---\")\n",
        "        print(result[\"text\"])\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during transcription: {e}\")\n",
        "        print(\"Please ensure your audio is clear and not too short.\")\n",
        "\n",
        "    # Optional: Clean up the recorded audio file after transcription\n",
        "    # os.remove(audio_file)\n",
        "    # print(f\"Removed temporary audio file: {audio_file}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peDTV4uO7Ouy",
        "outputId": "8a100e03-921e-48b2-a3ef-32e4b1048313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the entire process ---\n",
        "print(\"Ready to record and transcribe!\")\n",
        "transcribe_recorded_audio()"
      ],
      "metadata": {
        "id": "padcDOC_7VG_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "8787015d-170f-4b29-fafa-121ac8792f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to record and transcribe!\n",
            "Press 'Start Recording' to begin, 'Stop Recording' to finish.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = () => resolve(reader.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = async (time) => {\n",
              "  let stream       = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  let recorder     = new MediaRecorder(stream)\n",
              "  let div          = document.createElement('div')\n",
              "  let start        = document.createElement('button')\n",
              "  let stop         = document.createElement('button')\n",
              "  let recordedChunks = []\n",
              "  start.textContent = 'Start Recording'\n",
              "  stop.textContent  = 'Stop Recording'\n",
              "  div.appendChild(start)\n",
              "  div.appendChild(stop)\n",
              "  document.body.appendChild(div)\n",
              "\n",
              "  return new Promise(async resolve => {\n",
              "    start.onclick = () => {\n",
              "      recorder.start()\n",
              "      start.textContent = 'Recording...'\n",
              "      start.disabled = true;\n",
              "      stop.disabled = false;\n",
              "    }\n",
              "    stop.onclick =  () => {\n",
              "      recorder.stop()\n",
              "      start.textContent = 'Start Recording'\n",
              "      start.disabled = false;\n",
              "      stop.disabled = true;\n",
              "    }\n",
              "    recorder.ondataavailable = e => recordedChunks.push(e.data)\n",
              "    recorder.onstop = async () => {\n",
              "      let blob = new Blob(recordedChunks, { type: 'audio/wav' })\n",
              "      let url  = await b2text(blob)\n",
              "      resolve(url)\n",
              "    }\n",
              "  });\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio saved as 'recorded_audio_20260109_134905.wav'\n",
            "\n",
            "Loading Whisper model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 102MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whisper model loaded.\n",
            "\n",
            "Transcribing 'recorded_audio_20260109_134905.wav' with Whisper...\n",
            "\n",
            "--- Transcription Result ---\n",
            " agents in isolation, one using landchained, another using Creo AI, another build on OpenAIA. So if they all spoke different languages, none of them could collaborate. It would be like trying to run the internet without a standard like HTTP, complete chaos. That's why, standardized.\n"
          ]
        }
      ]
    }
  ]
}